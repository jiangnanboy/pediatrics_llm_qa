from threading import Thread
import os
import gradio as gr
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
from peft import PeftModel

os.environ["CUDA_VISIBLE_DEVICES"] = "1"

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

Model_PATH = '/Qwen2-1.5B-instruct'
LORA_CHECKPOINT_PATH = '/weights/checkpoint-lora'

def load_model_tokenizer():
    tokenizer = AutoTokenizer.from_pretrained(Model_PATH, use_fast=False, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(Model_PATH, device_map=DEVICE,
                                                 torch_dtype=torch.bfloat16).eval()

    # åŠ è½½è®­ç»ƒå¥½çš„Loraæ¨¡å‹ï¼Œå°†ä¸‹é¢çš„checkpointXXXæ›¿æ¢ä¸ºå®é™…çš„checkpointæ–‡ä»¶ååç§°
    model = PeftModel.from_pretrained(model, model_id=LORA_CHECKPOINT_PATH)

    model.generation_config.max_new_tokens = 512   # For chat.

    return model, tokenizer

def chat_stream(model, tokenizer, query, history):
    conversation = [
        {'role': 'system', 'content': "ä½ æ˜¯å„¿ç§‘ä¸“å®¶é—®ç­”åŠ©æ‰‹å°å˜‰ï¼Œä½ å°†å¸®åŠ©ç”¨æˆ·è§£ç­”åŸºç¡€çš„åŒ»ç–—é—®é¢˜ï¼Œä¸‹é¢æ˜¯ç”¨æˆ·æå‡ºçš„é—®é¢˜ï¼Œè¯·æ ¹æ®å®é™…åŒ»ç–—çŸ¥è¯†è¿›è¡Œå›ç­”ï¼Œ" \
                              "ç­”æ¡ˆåº”æ¡ç†æ¸…æ™°ï¼Œæœ‰å±‚æ¬¡ï¼Œä¸è¦æœ‰ä»»ä½•æœæ’°çš„å†…å®¹ï¼Œå¯¹äºæ— æ³•å›ç­”çš„é—®é¢˜ï¼Œè¯·ç”¨'æ‚¨çš„é—®é¢˜æˆ‘æš‚æ—¶æ— æ³•å›ç­”ï¼'è¿›è¡Œå›ç­”ã€‚"},
    ]
    for query_h, response_h in history:
        conversation.append({'role': 'user', 'content': query_h})
        conversation.append({'role': 'assistant', 'content': response_h})
    conversation.append({'role': 'user', 'content': query})
    inputs = tokenizer.apply_chat_template(
        conversation,
        add_generation_prompt=True,
        return_tensors='pt',
    )
    inputs = inputs.to(model.device)
    streamer = TextIteratorStreamer(tokenizer=tokenizer, skip_prompt=True, timeout=60.0, skip_special_tokens=True)
    generation_kwargs = dict(
        input_ids=inputs,
        streamer=streamer,
    )
    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()

    for new_text in streamer:
        yield new_text

def gc():
    import gc
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

def launch_demo(model, tokenizer):

    def predict(query, chatbot, task_history):
        print(f"User: {query}")
        chatbot.append((query, ""))
        full_response = ""
        response = ""
        for new_text in chat_stream(model, tokenizer, query, history=task_history):
            response += new_text
            chatbot[-1] = (query, response)

            yield chatbot
            full_response = response

        print(f"History: {task_history}")
        task_history.append((query, full_response))
        print(f"Instruct: {full_response}")

    def regenerate(chatbot, task_history):
        if not task_history:
            yield chatbot
            return
        item = task_history.pop(-1)
        chatbot.pop(-1)
        yield from predict(item[0], chatbot, task_history)

    def reset_user_input():
        return gr.update(value="")

    def reset_state(chatbot, task_history):
        task_history.clear()
        chatbot.clear()
        gc()
        return chatbot

    with gr.Blocks() as demo:
        gr.Markdown(
            """\
<center><font size=3>æœ¬é¡¹ç›®æ˜¯å„¿ç§‘é—®è¯Šå¯¹è¯æœºå™¨äºº, developed by Jiangnanboy.</center>""")
        chatbot = gr.Chatbot(label='XiaoJiaBot', elem_classes="control-height")
        query = gr.Textbox(lines=2, label='Input')
        task_history = gr.State([])

        with gr.Row():
            empty_btn = gr.Button("ğŸ§¹ Clear History (æ¸…é™¤å†å²)")
            submit_btn = gr.Button("ğŸš€ Submit (å‘é€)")
            regen_btn = gr.Button("ğŸ¤”ï¸ Regenerate (é‡è¯•)")

        submit_btn.click(predict, [query, chatbot, task_history], [chatbot], show_progress=True)
        submit_btn.click(reset_user_input, [], [query])
        empty_btn.click(reset_state, [chatbot, task_history], outputs=[chatbot], show_progress=True)
        regen_btn.click(regenerate, [chatbot, task_history], [chatbot], show_progress=True)

        gr.Markdown("""\
<font size=2>Note: This demo is governed by the original license of Jiangnanboy. \
We strongly advise users not to knowingly generate or allow others to knowingly generate harmful content, \
including hate speech, violence, pornography, deception, etc. \
(æ³¨ï¼šæœ¬æ¼”ç¤ºå—Jiangnanboyçš„è®¸å¯åè®®é™åˆ¶ã€‚æˆ‘ä»¬å¼ºçƒˆå»ºè®®ï¼Œç”¨æˆ·ä¸åº”ä¼ æ’­åŠä¸åº”å…è®¸ä»–äººä¼ æ’­ä»¥ä¸‹å†…å®¹ï¼Œ\
åŒ…æ‹¬ä½†ä¸é™äºä»‡æ¨è¨€è®ºã€æš´åŠ›ã€è‰²æƒ…ã€æ¬ºè¯ˆç›¸å…³çš„æœ‰å®³ä¿¡æ¯ã€‚)""")

    demo.queue().launch(
        share=False,
        inbrowser=False,
        server_port=8000,
        server_name='0.0.0.0',
    )

def main():
    model, tokenizer = load_model_tokenizer()
    launch_demo(model, tokenizer)

if __name__ == '__main__':
    main()